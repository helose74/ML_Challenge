{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas            as pd\n",
    "import numpy             as np\n",
    "import seaborn           as sb\n",
    "import scipy.stats       as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow        as tf\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습용 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Domain 특징값 추출 (10 features * 3 sensors = 30개씩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(x): # RMS 함수 정의\n",
    "    return np.sqrt(np.mean(x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoOfData    = 180\n",
    "NoOfSensor  = 3    # 가속도(Acceleration), 전압(Voltage), 전류(Current)\n",
    "NoOfFeature = 10   # 특징 개수:10개 (순서: Max, Min, Mean, RMS, Variance, Skewness, Kurtosis, Crest factor, Impulse factor, Shape factor)\n",
    "\n",
    "TimeFeature_Normal   = np.zeros((NoOfSensor*NoOfFeature , NoOfData))\n",
    "TimeFeature_Abnormal1 = np.zeros((NoOfSensor*NoOfFeature , NoOfData))\n",
    "TimeFeature_Abnormal2 = np.zeros((NoOfSensor*NoOfFeature , NoOfData))\n",
    "\n",
    "for i in range(NoOfData):\n",
    "    \n",
    "    # 데이터 불러오기\n",
    "    temp_path1 = './Train_Data/Normal_%d'%(i+1)   # Normal 데이터 파일 경로\n",
    "    temp_path2 = './Train_Data/Abnormal1_%d'%(i+1) # Abnormal 데이터 파일 경로\n",
    "    temp_path3 = './Train_Data/Abnormal2_%d'%(i+1) # Abnormal 데이터 파일 경로\n",
    "    temp_data1 = pd.read_csv(temp_path1 , sep=',' , header=None) # 임시 Normal 데이터\n",
    "    temp_data2 = pd.read_csv(temp_path2 , sep=',' , header=None) # 임시 Abnormal 데이터\n",
    "    temp_data3 = pd.read_csv(temp_path3 , sep=',' , header=None) # 임시 Abnormal 데이터\n",
    "    \n",
    "    # Time Domain 특징값 추출\n",
    "    for j in range(NoOfSensor):\n",
    "        \n",
    "        # Normal Time Domain Feature\n",
    "        TimeFeature_Normal[10*j+0, i] = np.max(temp_data1.iloc[:,j])\n",
    "        TimeFeature_Normal[10*j+1, i] = np.min(temp_data1.iloc[:,j])\n",
    "        TimeFeature_Normal[10*j+2, i] = np.mean(temp_data1.iloc[:,j])\n",
    "        TimeFeature_Normal[10*j+3, i] = rms(temp_data1.iloc[:,j])\n",
    "        TimeFeature_Normal[10*j+4, i] = np.var(temp_data1.iloc[:,j])\n",
    "        TimeFeature_Normal[10*j+5, i] = sp.skew(temp_data1.iloc[:,j])\n",
    "        TimeFeature_Normal[10*j+6, i] = sp.kurtosis(temp_data1.iloc[:,j])\n",
    "        TimeFeature_Normal[10*j+7, i] = np.max(temp_data1.iloc[:,j])/rms(temp_data1.iloc[:,j])\n",
    "        TimeFeature_Normal[10*j+8, i] = rms(temp_data1.iloc[:,j])/np.mean(temp_data1.iloc[:,j])\n",
    "        TimeFeature_Normal[10*j+9, i] = np.max(temp_data1.iloc[:,j])/np.mean(temp_data1.iloc[:,j])\n",
    "        \n",
    "        # Abnormal1 Time Domain Feature\n",
    "        TimeFeature_Abnormal1[10*j+0, i] = np.max(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal1[10*j+1, i] = np.min(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal1[10*j+2, i] = np.mean(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal1[10*j+3, i] = rms(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal1[10*j+4, i] = np.var(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal1[10*j+5, i] = sp.skew(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal1[10*j+6, i] = sp.kurtosis(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal1[10*j+7, i] = np.max(temp_data2.iloc[:,j])/rms(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal1[10*j+8, i] = rms(temp_data2.iloc[:,j])/np.mean(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal1[10*j+9, i] = np.max(temp_data2.iloc[:,j])/np.mean(temp_data2.iloc[:,j])\n",
    "        \n",
    "        # Abnormal2 Time Domain Feature\n",
    "        TimeFeature_Abnormal2[10*j+0, i] = np.max(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal2[10*j+1, i] = np.min(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal2[10*j+2, i] = np.mean(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal2[10*j+3, i] = rms(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal2[10*j+4, i] = np.var(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal2[10*j+5, i] = sp.skew(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal2[10*j+6, i] = sp.kurtosis(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal2[10*j+7, i] = np.max(temp_data2.iloc[:,j])/rms(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal2[10*j+8, i] = rms(temp_data2.iloc[:,j])/np.mean(temp_data2.iloc[:,j])\n",
    "        TimeFeature_Abnormal2[10*j+9, i] = np.max(temp_data2.iloc[:,j])/np.mean(temp_data2.iloc[:,j])\n",
    "        \n",
    "print(TimeFeature_Normal.shape)\n",
    "print(TimeFeature_Abnormal1.shape)\n",
    "print(TimeFeature_Abnormal2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Domain 특징값 추출 (10 features \\* 8 wavelet levels * 3 sensors = 240개씩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wavelet options\n",
    "\n",
    "MotherWavelet = pywt.Wavelet('haar')   # Mother wavelet (모함수) 지정\n",
    "Level   = 8                            # Wavelet 분해 레벨 지정\n",
    "select  = 8                            # 특징추출 영역 고주파 영역부터 개수 지정 (d1~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency Domain 특징값 추출 (Wavelet Transform 기반)\n",
    "FreqFeature_Normal   = np.zeros(shape=(NoOfSensor*NoOfFeature*select , NoOfData))\n",
    "FreqFeature_Abnormal1 = np.zeros(shape=(NoOfSensor*NoOfFeature*select , NoOfData))\n",
    "FreqFeature_Abnormal2 = np.zeros(shape=(NoOfSensor*NoOfFeature*select , NoOfData))\n",
    "\n",
    "for i in range(NoOfData):\n",
    "    \n",
    "    # 데이터 불러오기\n",
    "    temp_path1 = './Train_Data/Normal_%d'%(i+1)   # Normal 데이터 파일 경로\n",
    "    temp_path2 = './Train_Data/Abnormal1_%d'%(i+1) # Abnormal1 데이터 파일 경로\n",
    "    temp_path3 = './Train_Data/Abnormal2_%d'%(i+1) # Abnormal2 데이터 파일 경로\n",
    "    temp_data1 = pd.read_csv(temp_path1 , sep=',' , header=None) # 임시 Normal 데이터\n",
    "    temp_data2 = pd.read_csv(temp_path2 , sep=',' , header=None) # 임시 Abnormal1 데이터\n",
    "    temp_data3 = pd.read_csv(temp_path3 , sep=',' , header=None) # 임시 Abnormal2 데이터\n",
    "    Coef1      = pywt.wavedec(temp_data1, MotherWavelet, level=Level, axis=0)\n",
    "    Coef2      = pywt.wavedec(temp_data2, MotherWavelet, level=Level, axis=0)\n",
    "    Coef3      = pywt.wavedec(temp_data3, MotherWavelet, level=Level, axis=0)\n",
    "    \n",
    "    # Frequency Domain 특징값 추출\n",
    "    for j in range(NoOfSensor):\n",
    "        \n",
    "        for k in np.arange(select):\n",
    "            coef1 = Coef1[Level-k]\n",
    "            coef2 = Coef2[Level-k]\n",
    "            coef3 = Coef3[Level-k]\n",
    "            \n",
    "            # Normal Frequency Domain Feature\n",
    "            FreqFeature_Normal[NoOfFeature*j*select+k*NoOfFeature+0 , i] = np.max(coef1[:,j])\n",
    "            FreqFeature_Normal[NoOfFeature*j*select+k*NoOfFeature+1 , i] = np.min(coef1[:,j])\n",
    "            FreqFeature_Normal[NoOfFeature*j*select+k*NoOfFeature+2 , i] = np.mean(coef1[:,j])\n",
    "            FreqFeature_Normal[NoOfFeature*j*select+k*NoOfFeature+3 , i] = np.var(coef1[:,j])\n",
    "            FreqFeature_Normal[NoOfFeature*j*select+k*NoOfFeature+4 , i] = rms(coef1[:,j])\n",
    "            FreqFeature_Normal[NoOfFeature*j*select+k*NoOfFeature+5 , i] = sp.skew(coef1[:,j])\n",
    "            FreqFeature_Normal[NoOfFeature*j*select+k*NoOfFeature+6 , i] = sp.kurtosis(coef1[:,j])\n",
    "            FreqFeature_Normal[NoOfFeature*j*select+k*NoOfFeature+7 , i] = np.max(coef1[:,j])/rms(coef1[:,j])\n",
    "            FreqFeature_Normal[NoOfFeature*j*select+k*NoOfFeature+8 , i] = rms(coef1[:,j])/np.mean(coef1[:,j])\n",
    "            FreqFeature_Normal[NoOfFeature*j*select+k*NoOfFeature+9 , i] = np.max(coef1[:,j])/np.mean(coef1[:,j])\n",
    "            \n",
    "            # Abnormal1 Frequency Domain Feature\n",
    "            FreqFeature_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+0 , i] = np.max(coef2[:,j])\n",
    "            FreqFeature_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+1 , i] = np.min(coef2[:,j])\n",
    "            FreqFeature_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+2 , i] = np.mean(coef2[:,j])\n",
    "            FreqFeature_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+3 , i] = np.var(coef2[:,j])\n",
    "            FreqFeature_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+4 , i] = rms(coef2[:,j])\n",
    "            FreqFeature_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+5 , i] = sp.skew(coef2[:,j])\n",
    "            FreqFeature_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+6 , i] = sp.kurtosis(coef2[:,j])\n",
    "            FreqFeature_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+7 , i] = np.max(coef2[:,j])/rms(coef2[:,j])\n",
    "            FreqFeature_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+8 , i] = rms(coef2[:,j])/np.mean(coef2[:,j])\n",
    "            FreqFeature_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+9 , i] = np.max(coef2[:,j])/np.mean(coef2[:,j])\n",
    "            \n",
    "            # Abnormal2 Frequency Domain Feature\n",
    "            FreqFeature_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+0 , i] = np.max(coef3[:,j])\n",
    "            FreqFeature_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+1 , i] = np.min(coef3[:,j])\n",
    "            FreqFeature_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+2 , i] = np.mean(coef3[:,j])\n",
    "            FreqFeature_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+3 , i] = np.var(coef3[:,j])\n",
    "            FreqFeature_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+4 , i] = rms(coef3[:,j])\n",
    "            FreqFeature_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+5 , i] = sp.skew(coef3[:,j])\n",
    "            FreqFeature_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+6 , i] = sp.kurtosis(coef3[:,j])\n",
    "            FreqFeature_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+7 , i] = np.max(coef3[:,j])/rms(coef3[:,j])\n",
    "            FreqFeature_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+8 , i] = rms(coef3[:,j])/np.mean(coef3[:,j])\n",
    "            FreqFeature_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+9 , i] = np.max(coef3[:,j])/np.mean(coef3[:,j])\n",
    "\n",
    "print(FreqFeature_Normal.shape)\n",
    "print(FreqFeature_Abnormal1.shape)\n",
    "print(FreqFeature_Abnormal2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 통합 (Time domain 30개 + Freq. domain 240 = 270개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_Normal = np.concatenate([TimeFeature_Normal,FreqFeature_Normal] , axis=0)\n",
    "Feature_Abnormal1 = np.concatenate([TimeFeature_Abnormal1,FreqFeature_Abnormal1] , axis=0)\n",
    "Feature_Abnormal2 = np.concatenate([TimeFeature_Abnormal2,FreqFeature_Abnormal2] , axis=0)\n",
    "Normal_FeatureData = pd.DataFrame(Feature_Normal)\n",
    "Abnormal1_FeatureData = pd.DataFrame(Feature_Abnormal1)\n",
    "Abnormal2_FeatureData = pd.DataFrame(Feature_Abnormal2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-value 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoOfFeature = Feature_Normal.shape[0] # 추출된 Feature 갯수\n",
    "\n",
    "P_value = np.zeros((NoOfFeature , 2))\n",
    "\n",
    "# 특징값 각각 Anova검정 수행\n",
    "for i in np.arange(NoOfFeature):\n",
    "    \n",
    "    Anova_test       = np.array(sp.f_oneway(Normal_FeatureData.iloc[i,:] , Abnormal1_FeatureData.iloc[i,:] , Abnormal2_FeatureData.iloc[i,:]))\n",
    "    P_value[i,0] = i          # Feature Index\n",
    "    P_value[i,1] = Anova_test[1]  # P값 (P-value)\n",
    "    \n",
    "P_value      = pd.DataFrame(P_value)\n",
    "P_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_value_Rank = P_value.sort_values([1],ascending=True)  # P-value 기준 오름차순 정렬\n",
    "\n",
    "P_value_Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Data/P_value_Rank'   # path = '파일 경로/저장할 파일 이름'\n",
    "P_value_Rank.to_csv(path, sep=',', header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P value 기준 차원축소 대상 데이터(Feature data) 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number    = 20\n",
    "\n",
    "Normal_SelectedFeatues = np.zeros((Number,NoOfData))\n",
    "Abnormal1_SelectedFeatues = np.zeros((Number,NoOfData))\n",
    "Abnormal2_SelectedFeatues = np.zeros((Number,NoOfData))\n",
    "\n",
    "for i in range(Number):\n",
    "    \n",
    "    index                 = int(P_value_Rank.iloc[i,0])\n",
    "    Normal_SelectedFeatues[i,:]   = Normal_FeatureData.iloc[index,:].values\n",
    "    Abnormal1_SelectedFeatues[i,:] = Abnormal1_FeatureData.iloc[index,:].values\n",
    "    Abnormal2_SelectedFeatues[i,:] = Abnormal2_FeatureData.iloc[index,:].values\n",
    "\n",
    "# 정상, 고장 특징값 합치기    \n",
    "FeatureSelected = pd.DataFrame(np.concatenate([Normal_SelectedFeatues, Abnormal1_SelectedFeatues, Abnormal2_SelectedFeatues] , axis=1))\n",
    "FeatureSelected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가용 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoOfTestData    = 20\n",
    "NoOfSensor  = 3    # 가속도(Acceleration), 전압(Voltage), 전류(Current)\n",
    "NoOfFeature = 10   # 특징 개수:10개 (순서: Max, Min, Mean, RMS, Variance, Skewness, Kurtosis, Crest factor, Impulse factor, Shape factor)\n",
    "\n",
    "TimeFeatureTest_Normal   = np.zeros((NoOfSensor*NoOfFeature , NoOfTestData))\n",
    "TimeFeatureTest_Abnormal1 = np.zeros((NoOfSensor*NoOfFeature , NoOfTestData))\n",
    "TimeFeatureTest_Abnormal2 = np.zeros((NoOfSensor*NoOfFeature , NoOfTestData))\n",
    "\n",
    "for i in range(NoOfTestData):\n",
    "    \n",
    "    # 데이터 불러오기\n",
    "    temp_test_path1 = './Test_Data/Normal_%d'%(i+1)   # Normal 데이터 파일 경로\n",
    "    temp_test_path2 = './Test_Data/Abnormal1_%d'%(i+1) # Abnormal1 데이터 파일 경로\n",
    "    temp_test_path3 = './Test_Data/Abnormal2_%d'%(i+1) # Abnormal2 데이터 파일 경로\n",
    "    temp_test_data1 = pd.read_csv(temp_test_path1 , sep=',' , header=None) # 임시 Normal 데이터\n",
    "    temp_test_data2 = pd.read_csv(temp_test_path2 , sep=',' , header=None) # 임시 Abnormal 데이터\n",
    "    temp_test_data3 = pd.read_csv(temp_test_path3 , sep=',' , header=None) # 임시 Abnormal 데이터\n",
    "    \n",
    "    # Time Domain 특징값 추출\n",
    "    for j in range(NoOfSensor):\n",
    "        \n",
    "        # Normal Time Domain Feature\n",
    "        TimeFeatureTest_Normal[10*j+0, i] = np.max(temp_test_data1.iloc[:,j])\n",
    "        TimeFeatureTest_Normal[10*j+1, i] = np.min(temp_test_data1.iloc[:,j])\n",
    "        TimeFeatureTest_Normal[10*j+2, i] = np.mean(temp_test_data1.iloc[:,j])\n",
    "        TimeFeatureTest_Normal[10*j+3, i] = rms(temp_test_data1.iloc[:,j])\n",
    "        TimeFeatureTest_Normal[10*j+4, i] = np.var(temp_test_data1.iloc[:,j])\n",
    "        TimeFeatureTest_Normal[10*j+5, i] = sp.skew(temp_test_data1.iloc[:,j])\n",
    "        TimeFeatureTest_Normal[10*j+6, i] = sp.kurtosis(temp_test_data1.iloc[:,j])\n",
    "        TimeFeatureTest_Normal[10*j+7, i] = np.max(temp_test_data1.iloc[:,j])/rms(temp_test_data1.iloc[:,j])\n",
    "        TimeFeatureTest_Normal[10*j+8, i] = rms(temp_test_data1.iloc[:,j])/np.mean(temp_test_data1.iloc[:,j])\n",
    "        TimeFeatureTest_Normal[10*j+9, i] = np.max(temp_test_data1.iloc[:,j])/np.mean(temp_test_data1.iloc[:,j])\n",
    "        \n",
    "        # Abnormal1 Time Domain Feature\n",
    "        TimeFeatureTest_Abnormal1[10*j+0, i] = np.max(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal1[10*j+1, i] = np.min(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal1[10*j+2, i] = np.mean(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal1[10*j+3, i] = rms(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal1[10*j+4, i] = np.var(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal1[10*j+5, i] = sp.skew(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal1[10*j+6, i] = sp.kurtosis(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal1[10*j+7, i] = np.max(temp_test_data2.iloc[:,j])/rms(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal1[10*j+8, i] = rms(temp_test_data2.iloc[:,j])/np.mean(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal1[10*j+9, i] = np.max(temp_test_data2.iloc[:,j])/np.mean(temp_test_data2.iloc[:,j])\n",
    "        \n",
    "        # Abnormal2 Time Domain Feature\n",
    "        TimeFeatureTest_Abnormal2[10*j+0, i] = np.max(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal2[10*j+1, i] = np.min(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal2[10*j+2, i] = np.mean(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal2[10*j+3, i] = rms(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal2[10*j+4, i] = np.var(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal2[10*j+5, i] = sp.skew(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal2[10*j+6, i] = sp.kurtosis(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal2[10*j+7, i] = np.max(temp_test_data2.iloc[:,j])/rms(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal2[10*j+8, i] = rms(temp_test_data2.iloc[:,j])/np.mean(temp_test_data2.iloc[:,j])\n",
    "        TimeFeatureTest_Abnormal2[10*j+9, i] = np.max(temp_test_data2.iloc[:,j])/np.mean(temp_test_data2.iloc[:,j])\n",
    "        \n",
    "print(TimeFeatureTest_Normal.shape)\n",
    "print(TimeFeatureTest_Abnormal1.shape)\n",
    "print(TimeFeatureTest_Abnormal2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency Domain 특징값 추출 (Wavelet Transform 기반)\n",
    "FreqFeatureTest_Normal   = np.zeros(shape=(NoOfSensor*NoOfFeature*select , NoOfTestData))\n",
    "FreqFeatureTest_Abnormal1 = np.zeros(shape=(NoOfSensor*NoOfFeature*select , NoOfTestData))\n",
    "FreqFeatureTest_Abnormal2 = np.zeros(shape=(NoOfSensor*NoOfFeature*select , NoOfTestData))\n",
    "\n",
    "for i in range(NoOfTestData):\n",
    "    \n",
    "    # 데이터 불러오기\n",
    "    temp_test_path1 = './Test_Data/Normal_%d'%(i+1)   # Normal 데이터 파일 경로\n",
    "    temp_test_path2 = './Test_Data/Abnormal1_%d'%(i+1) # Abnormal1 데이터 파일 경로\n",
    "    temp_test_path3 = './Test_Data/Abnormal2_%d'%(i+1) # Abnormal2 데이터 파일 경로\n",
    "    temp_test_data1 = pd.read_csv(temp_test_path1 , sep=',' , header=None) # 임시 Normal 데이터\n",
    "    temp_test_data2 = pd.read_csv(temp_test_path2 , sep=',' , header=None) # 임시 Abnormal1 데이터\n",
    "    temp_test_data3 = pd.read_csv(temp_test_path3 , sep=',' , header=None) # 임시 Abnormal2 데이터\n",
    "    Coef1      = pywt.wavedec(temp_test_data1, MotherWavelet, level=Level, axis=0)\n",
    "    Coef2      = pywt.wavedec(temp_test_data2, MotherWavelet, level=Level, axis=0)\n",
    "    Coef3      = pywt.wavedec(temp_test_data3, MotherWavelet, level=Level, axis=0)\n",
    "    \n",
    "    # Frequency Domain 특징값 추출\n",
    "    for j in range(NoOfSensor):\n",
    "        \n",
    "        for k in np.arange(select):\n",
    "            coef1 = Coef1[Level-k]\n",
    "            coef2 = Coef2[Level-k]\n",
    "            coef3 = Coef3[Level-k]\n",
    "            \n",
    "            # Normal Frequency Domain Feature\n",
    "            FreqFeatureTest_Normal[NoOfFeature*j*select+k*NoOfFeature+0 , i] = np.max(coef1[:,j])\n",
    "            FreqFeatureTest_Normal[NoOfFeature*j*select+k*NoOfFeature+1 , i] = np.min(coef1[:,j])\n",
    "            FreqFeatureTest_Normal[NoOfFeature*j*select+k*NoOfFeature+2 , i] = np.mean(coef1[:,j])\n",
    "            FreqFeatureTest_Normal[NoOfFeature*j*select+k*NoOfFeature+3 , i] = np.var(coef1[:,j])\n",
    "            FreqFeatureTest_Normal[NoOfFeature*j*select+k*NoOfFeature+4 , i] = rms(coef1[:,j])\n",
    "            FreqFeatureTest_Normal[NoOfFeature*j*select+k*NoOfFeature+5 , i] = sp.skew(coef1[:,j])\n",
    "            FreqFeatureTest_Normal[NoOfFeature*j*select+k*NoOfFeature+6 , i] = sp.kurtosis(coef1[:,j])\n",
    "            FreqFeatureTest_Normal[NoOfFeature*j*select+k*NoOfFeature+7 , i] = np.max(coef1[:,j])/rms(coef1[:,j])\n",
    "            FreqFeatureTest_Normal[NoOfFeature*j*select+k*NoOfFeature+8 , i] = rms(coef1[:,j])/np.mean(coef1[:,j])\n",
    "            FreqFeatureTest_Normal[NoOfFeature*j*select+k*NoOfFeature+9 , i] = np.max(coef1[:,j])/np.mean(coef1[:,j])\n",
    "            \n",
    "            # Abnormal1 Frequency Domain Feature\n",
    "            FreqFeatureTest_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+0 , i] = np.max(coef2[:,j])\n",
    "            FreqFeatureTest_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+1 , i] = np.min(coef2[:,j])\n",
    "            FreqFeatureTest_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+2 , i] = np.mean(coef2[:,j])\n",
    "            FreqFeatureTest_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+3 , i] = np.var(coef2[:,j])\n",
    "            FreqFeatureTest_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+4 , i] = rms(coef2[:,j])\n",
    "            FreqFeatureTest_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+5 , i] = sp.skew(coef2[:,j])\n",
    "            FreqFeatureTest_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+6 , i] = sp.kurtosis(coef2[:,j])\n",
    "            FreqFeatureTest_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+7 , i] = np.max(coef2[:,j])/rms(coef2[:,j])\n",
    "            FreqFeatureTest_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+8 , i] = rms(coef2[:,j])/np.mean(coef2[:,j])\n",
    "            FreqFeatureTest_Abnormal1[NoOfFeature*j*select+k*NoOfFeature+9 , i] = np.max(coef2[:,j])/np.mean(coef2[:,j])\n",
    "            \n",
    "            # Abnormal2 Frequency Domain Feature\n",
    "            FreqFeatureTest_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+0 , i] = np.max(coef3[:,j])\n",
    "            FreqFeatureTest_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+1 , i] = np.min(coef3[:,j])\n",
    "            FreqFeatureTest_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+2 , i] = np.mean(coef3[:,j])\n",
    "            FreqFeatureTest_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+3 , i] = np.var(coef3[:,j])\n",
    "            FreqFeatureTest_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+4 , i] = rms(coef3[:,j])\n",
    "            FreqFeatureTest_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+5 , i] = sp.skew(coef3[:,j])\n",
    "            FreqFeatureTest_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+6 , i] = sp.kurtosis(coef3[:,j])\n",
    "            FreqFeatureTest_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+7 , i] = np.max(coef3[:,j])/rms(coef3[:,j])\n",
    "            FreqFeatureTest_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+8 , i] = rms(coef3[:,j])/np.mean(coef3[:,j])\n",
    "            FreqFeatureTest_Abnormal2[NoOfFeature*j*select+k*NoOfFeature+9 , i] = np.max(coef3[:,j])/np.mean(coef3[:,j])\n",
    "\n",
    "print(FreqFeatureTest_Normal.shape)\n",
    "print(FreqFeatureTest_Abnormal1.shape)\n",
    "print(FreqFeatureTest_Abnormal2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Normal = np.concatenate([TimeFeatureTest_Normal,FreqFeatureTest_Normal] , axis=0)\n",
    "Test_Abnormal1 = np.concatenate([TimeFeatureTest_Abnormal1,FreqFeatureTest_Abnormal1] , axis=0)\n",
    "Test_Abnormal2 = np.concatenate([TimeFeatureTest_Abnormal2,FreqFeatureTest_Abnormal2] , axis=0)\n",
    "Normal_TestData = pd.DataFrame(Test_Normal)\n",
    "Abnormal1_TestData = pd.DataFrame(Test_Abnormal1)\n",
    "Abnormal2_TestData = pd.DataFrame(Test_Abnormal2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number = 20\n",
    "\n",
    "Normal_SelectedTestFeatues   = np.zeros((Number,NoOfTestData))\n",
    "Abnormal1_SelectedTestFeatues = np.zeros((Number,NoOfTestData))\n",
    "Abnormal2_SelectedTestFeatues = np.zeros((Number,NoOfTestData))\n",
    "\n",
    "for i in range(Number):\n",
    "    \n",
    "    index         = int(P_value_Rank.iloc[i,0])\n",
    "    Normal_SelectedTestFeatues[i,:]   = Normal_TestData.iloc[index,:].values\n",
    "    Abnormal1_SelectedTestFeatues[i,:] = Abnormal1_TestData.iloc[index,:].values\n",
    "    Abnormal2_SelectedTestFeatues[i,:] = Abnormal2_TestData.iloc[index,:].values\n",
    "\n",
    "# 정상, 고장 특징값 합치기    \n",
    "TestFeatureSelected = pd.DataFrame(np.concatenate([Normal_SelectedTestFeatues, Abnormal1_SelectedTestFeatues, Abnormal2_SelectedTestFeatues] , axis=1))\n",
    "TestFeatureSelected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureSelected = pd.DataFrame(np.transpose(FeatureSelected))\n",
    "FeatureSelected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_All = FeatureSelected.iloc[:,:]\n",
    "Training_All.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoOfTrainData = int(Training_All.shape[0]/3)\n",
    "print(NoOfTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_All_Label_forANN = np.zeros((NoOfTrainData*3,3), dtype=int)\n",
    "\n",
    "Training_All_Label_forANN[:NoOfTrainData,0] = 1\n",
    "Training_All_Label_forANN[NoOfTrainData:NoOfTrainData*2 , 1] = 1\n",
    "Training_All_Label_forANN[NoOfTrainData*2:,2] = 1\n",
    "Training_All_Label_forANN = pd.DataFrame(Training_All_Label_forANN)\n",
    "\n",
    "Training_All_Label_forANN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestFeatureSelected = pd.DataFrame(np.transpose(TestFeatureSelected))\n",
    "TestFeatureSelected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_All = TestFeatureSelected.iloc[:,:]\n",
    "Test_All.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_All_Label_forANN = np.zeros((NoOfTestData*3,3), dtype=int)\n",
    "\n",
    "Test_All_Label_forANN[:NoOfTestData,0] = 1\n",
    "Test_All_Label_forANN[NoOfTestData:NoOfTestData*2 , 1] = 1\n",
    "Test_All_Label_forANN[NoOfTestData*2:,2] = 1\n",
    "Test_All_Label_forANN = pd.DataFrame(Test_All_Label_forANN)\n",
    "\n",
    "Test_All_Label_forANN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_All_Label_forANN.to_csv('./Data/Test_Label_forANN', sep = \",\", header = None, index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN hyper-parameter 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.random.set_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate1  = 0.01\n",
    "learningRate2  = 0.005\n",
    "learningRate3  = 0.001\n",
    "\n",
    "noOfNeuron1    = 4\n",
    "noOfNeuron2    = 8\n",
    "noOfNeuron3    = 16\n",
    "\n",
    "iteration    = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data  = np.array(Training_All)\n",
    "Label = np.array(Training_All_Label_forANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_All  = np.array(Training_All)\n",
    "Training_All_Label_forANN = np.array(Test_All_Label_forANN)\n",
    "Test_All  = np.array(Test_All)\n",
    "Test_All_Label_forANN = np.array(Test_All_Label_forANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model1 (learningRate  = 0.01, noOfNeuron    = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_model1(input_data):\n",
    "    model1 = keras.Sequential()\n",
    "    model1.add(keras.layers.Dense(units = noOfNeuron1,\n",
    "                                 input_shape = (input_data.shape[1],), activation = 'relu'))    # Input  Layer\n",
    "    model1.add(keras.layers.Dense(units = noOfNeuron1,                 activation = 'relu'))    # Hidden Layer 1\n",
    "    model1.add(keras.layers.Dense(units = noOfNeuron1,                 activation = 'relu'))    # Hidden Layer 2\n",
    "    model1.add(keras.layers.Dense(units = 3,                           activation = 'softmax')) # Output Layer\n",
    "    \n",
    "    \n",
    "    \n",
    "    model1.compile(optimizer= keras.optimizers.Adam(learning_rate = learningRate1),\n",
    "                  loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(777)    \n",
    "\n",
    "model1 = ANN_model1(Training_All)\n",
    "hist1 = model1.fit(Data, Label, epochs=iteration, verbose = 0)\n",
    "Loss1, Accuracy1 = model1.evaluate(Test_All, Test_All_Label_forANN, verbose=0)\n",
    "\n",
    "Accuracy1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model2 (learningRate  = 0.01, noOfNeuron    = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_model2(input_data):\n",
    "    model2 = keras.Sequential()\n",
    "    model2.add(keras.layers.Dense(units = noOfNeuron2,\n",
    "                                 input_shape = (input_data.shape[1],), activation = 'relu'))    # Input  Layer\n",
    "    model2.add(keras.layers.Dense(units = noOfNeuron2,                 activation = 'relu'))    # Hidden Layer 1\n",
    "    model2.add(keras.layers.Dense(units = noOfNeuron2,                 activation = 'relu'))    # Hidden Layer 2\n",
    "    model2.add(keras.layers.Dense(units = 3,                           activation = 'softmax')) # Output Layer\n",
    "    \n",
    "    \n",
    "    \n",
    "    model2.compile(optimizer= keras.optimizers.Adam(learning_rate = learningRate1),\n",
    "                  loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(777)    \n",
    "\n",
    "model2 = ANN_model2(Training_All)\n",
    "hist2 = model2.fit(Data, Label, epochs=iteration, verbose = 0)\n",
    "Loss2, Accuracy2 = model2.evaluate(Test_All, Test_All_Label_forANN, verbose=0)\n",
    "\n",
    "Accuracy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model3 (learningRate  = 0.01, noOfNeuron    = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_model3(input_data):\n",
    "    model3 = keras.Sequential()\n",
    "    model3.add(keras.layers.Dense(units = noOfNeuron3,\n",
    "                                 input_shape = (input_data.shape[1],), activation = 'relu'))    # Input  Layer\n",
    "    model3.add(keras.layers.Dense(units = noOfNeuron3,                 activation = 'relu'))    # Hidden Layer 1\n",
    "    model3.add(keras.layers.Dense(units = noOfNeuron3,                 activation = 'relu'))    # Hidden Layer 2\n",
    "    model3.add(keras.layers.Dense(units = 3,                           activation = 'softmax')) # Output Layer\n",
    "    \n",
    "    \n",
    "    \n",
    "    model3.compile(optimizer= keras.optimizers.Adam(learning_rate = learningRate1),\n",
    "                  loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(777)    \n",
    "\n",
    "model3 = ANN_model3(Training_All)\n",
    "hist3 = model3.fit(Data, Label, epochs=iteration, verbose = 0)\n",
    "Loss3, Accuracy3 = model3.evaluate(Test_All, Test_All_Label_forANN, verbose=0)\n",
    "\n",
    "Accuracy3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model4 (learningRate  = 0.005, noOfNeuron    = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_model4(input_data):\n",
    "    model4 = keras.Sequential()\n",
    "    model4.add(keras.layers.Dense(units = noOfNeuron1,\n",
    "                                 input_shape = (input_data.shape[1],), activation = 'relu'))    # Input  Layer\n",
    "    model4.add(keras.layers.Dense(units = noOfNeuron1,                 activation = 'relu'))    # Hidden Layer 1\n",
    "    model4.add(keras.layers.Dense(units = noOfNeuron1,                 activation = 'relu'))    # Hidden Layer 2\n",
    "    model4.add(keras.layers.Dense(units = 3,                           activation = 'softmax')) # Output Layer\n",
    "    \n",
    "    \n",
    "    \n",
    "    model4.compile(optimizer= keras.optimizers.Adam(learning_rate = learningRate2),\n",
    "                  loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    return model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(777)    \n",
    "\n",
    "model4 = ANN_model4(Training_All)\n",
    "hist4 = model4.fit(Data, Label, epochs=iteration, verbose = 0)\n",
    "Loss4, Accuracy4 = model4.evaluate(Test_All, Test_All_Label_forANN, verbose=0)\n",
    "\n",
    "Accuracy4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model5 (learningRate  = 0.005, noOfNeuron    = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_model5(input_data):\n",
    "    model5 = keras.Sequential()\n",
    "    model5.add(keras.layers.Dense(units = noOfNeuron2,\n",
    "                                 input_shape = (input_data.shape[1],), activation = 'relu'))    # Input  Layer\n",
    "    model5.add(keras.layers.Dense(units = noOfNeuron2,                 activation = 'relu'))    # Hidden Layer 1\n",
    "    model5.add(keras.layers.Dense(units = noOfNeuron2,                 activation = 'relu'))    # Hidden Layer 2\n",
    "    model5.add(keras.layers.Dense(units = 3,                           activation = 'softmax')) # Output Layer\n",
    "    \n",
    "    \n",
    "    \n",
    "    model5.compile(optimizer= keras.optimizers.Adam(learning_rate = learningRate2),\n",
    "                  loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    return model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(777)    \n",
    "\n",
    "model5 = ANN_model5(Training_All)\n",
    "hist5 = model5.fit(Data, Label, epochs=iteration, verbose = 0)\n",
    "Loss5, Accuracy5 = model5.evaluate(Test_All, Test_All_Label_forANN, verbose=0)\n",
    "\n",
    "Accuracy5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model6 (learningRate  = 0.005, noOfNeuron    = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_model6(input_data):\n",
    "    model6 = keras.Sequential()\n",
    "    model6.add(keras.layers.Dense(units = noOfNeuron3,\n",
    "                                 input_shape = (input_data.shape[1],), activation = 'relu'))    # Input  Layer\n",
    "    model6.add(keras.layers.Dense(units = noOfNeuron3,                 activation = 'relu'))    # Hidden Layer 1\n",
    "    model6.add(keras.layers.Dense(units = noOfNeuron3,                 activation = 'relu'))    # Hidden Layer 2\n",
    "    model6.add(keras.layers.Dense(units = 3,                           activation = 'softmax')) # Output Layer\n",
    "    \n",
    "    \n",
    "    \n",
    "    model6.compile(optimizer= keras.optimizers.Adam(learning_rate = learningRate2),\n",
    "                  loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    return model6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(777)    \n",
    "\n",
    "model6 = ANN_model6(Training_All)\n",
    "hist6 = model6.fit(Data, Label, epochs=iteration, verbose = 0)\n",
    "Loss6, Accuracy6 = model6.evaluate(Test_All, Test_All_Label_forANN, verbose=0)\n",
    "\n",
    "Accuracy6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model7 (learningRate  = 0.001, noOfNeuron    = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_model7(input_data):\n",
    "    model7 = keras.Sequential()\n",
    "    model7.add(keras.layers.Dense(units = noOfNeuron1,\n",
    "                                 input_shape = (input_data.shape[1],), activation = 'relu'))    # Input  Layer\n",
    "    model7.add(keras.layers.Dense(units = noOfNeuron1,                 activation = 'relu'))    # Hidden Layer 1\n",
    "    model7.add(keras.layers.Dense(units = noOfNeuron1,                 activation = 'relu'))    # Hidden Layer 2\n",
    "    model7.add(keras.layers.Dense(units = 3,                           activation = 'softmax')) # Output Layer\n",
    "    \n",
    "    \n",
    "    \n",
    "    model7.compile(optimizer= keras.optimizers.Adam(learning_rate = learningRate3),\n",
    "                  loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    return model7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(777)    \n",
    "\n",
    "model7 = ANN_model7(Training_All)\n",
    "hist7 = model7.fit(Data, Label, epochs=iteration, verbose = 0)\n",
    "Loss7, Accuracy7 = model7.evaluate(Test_All, Test_All_Label_forANN, verbose=0)\n",
    "\n",
    "Accuracy7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model8 (learningRate  = 0.001, noOfNeuron    = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_model8(input_data):\n",
    "    model8 = keras.Sequential()\n",
    "    model8.add(keras.layers.Dense(units = noOfNeuron2,\n",
    "                                 input_shape = (input_data.shape[1],), activation = 'relu'))    # Input  Layer\n",
    "    model8.add(keras.layers.Dense(units = noOfNeuron2,                 activation = 'relu'))    # Hidden Layer 1\n",
    "    model8.add(keras.layers.Dense(units = noOfNeuron2,                 activation = 'relu'))    # Hidden Layer 2\n",
    "    model8.add(keras.layers.Dense(units = 3,                           activation = 'softmax')) # Output Layer\n",
    "    \n",
    "    \n",
    "    \n",
    "    model8.compile(optimizer= keras.optimizers.Adam(learning_rate = learningRate3),\n",
    "                  loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    return model8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(777)    \n",
    "\n",
    "model8 = ANN_model8(Training_All)\n",
    "hist8 = model8.fit(Data, Label, epochs=iteration, verbose = 0)\n",
    "Loss8, Accuracy8 = model8.evaluate(Test_All, Test_All_Label_forANN, verbose=0)\n",
    "\n",
    "Accuracy8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model9 (learningRate  = 0.001, noOfNeuron    = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_model9(input_data):\n",
    "    model9 = keras.Sequential()\n",
    "    model9.add(keras.layers.Dense(units = noOfNeuron3,\n",
    "                                 input_shape = (input_data.shape[1],), activation = 'relu'))    # Input  Layer\n",
    "    model9.add(keras.layers.Dense(units = noOfNeuron3,                 activation = 'relu'))    # Hidden Layer 1\n",
    "    model9.add(keras.layers.Dense(units = noOfNeuron3,                 activation = 'relu'))    # Hidden Layer 2\n",
    "    model9.add(keras.layers.Dense(units = 3,                           activation = 'softmax')) # Output Layer\n",
    "    \n",
    "    \n",
    "    \n",
    "    model9.compile(optimizer= keras.optimizers.Adam(learning_rate = learningRate3),\n",
    "                  loss=keras.losses.categorical_crossentropy,\n",
    "                  metrics=['accuracy'])\n",
    "    return model9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(777)    \n",
    "\n",
    "model9 = ANN_model9(Training_All)\n",
    "hist9 = model9.fit(Data, Label, epochs=iteration, verbose = 0)\n",
    "Loss9, Accuracy9 = model9.evaluate(Test_All, Test_All_Label_forANN, verbose=0)\n",
    "\n",
    "Accuracy9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 모델 진단 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    s1 = \"Accuracy%d = round(Accuracy%d , 2)\"%(i+1,i+1)\n",
    "    exec(s1)\n",
    "\n",
    "Acc = [[Accuracy1, Accuracy2, Accuracy3], [Accuracy4, Accuracy5, Accuracy6], [Accuracy7, Accuracy8, Accuracy9]]\n",
    "Acc_df = pd.DataFrame(Acc, index=['0.01', '0.005', '0.001'],columns= ['4', '8', '16'])\n",
    "Acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.save(\"./Data/ANN_Bestmodel.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
